:py:mod:`torchapprox.operators.conv2d`
======================================

.. py:module:: torchapprox.operators.conv2d


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   torchapprox.operators.conv2d.Conv2dArgs
   torchapprox.operators.conv2d.ApproxConv2dOp
   torchapprox.operators.conv2d.FastApproxConv2dOp
   torchapprox.operators.conv2d.ApproxDWConv2dOp



Functions
~~~~~~~~~

.. autoapisummary::

   torchapprox.operators.conv2d._conv_bwd_ste



.. py:class:: Conv2dArgs

   Container class to pass convolution parameters
   around in a convenient way

   .. py:attribute:: in_channels
      :annotation: :int

      

   .. py:attribute:: out_channels
      :annotation: :int

      

   .. py:attribute:: kernel_size
      :annotation: :Union[int, Tuple[int, int]]

      

   .. py:attribute:: stride
      :annotation: :Union[int, Tuple[int, int]]

      

   .. py:attribute:: padding
      :annotation: :Union[int, Tuple[int, int]]

      

   .. py:attribute:: dilation
      :annotation: :Union[int, Tuple[int, int]]

      

   .. py:attribute:: groups
      :annotation: :int

      

   .. py:method:: backward_args() -> Dict[str, Any]

      Generate arguments required by backward pass
      for gradient calculation

      :returns: Dict populated with parameters required
                in backward pass



.. py:function:: _conv_bwd_ste(grad, x, w, conf)

   Wrapper to reuse Conv2d gradient calculation
   for several approximate forward functions

   :param grad: Upstream gradient
   :param x: Activations from forward pass
   :param w: Weight from forward pass
   :param conf: Conv2d parameters

   :returns: Gradients for activations and weights


.. py:class:: ApproxConv2dOp(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`

   Autograd wrapper around Im2Col/ApproxGeMM Conv2d operator

   .. py:method:: forward(ctx, x, w, conv_args: Conv2dArgs, out_dims, lut)
      :staticmethod:

      Performs the operation.

      This function is to be overridden by all subclasses.

      It must accept a context ctx as the first argument, followed by any
      number of arguments (tensors or other types).

      The context can be used to store arbitrary data that can be then
      retrieved during the backward pass. Tensors should not be stored
      directly on `ctx` (though this is not currently enforced for
      backward compatibility). Instead, tensors should be saved either with
      :func:`ctx.save_for_backward` if they are intended to be used in
      ``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`
      if they are intended to be used for in ``jvp``.


   .. py:method:: backward(ctx, grad)
      :staticmethod:

      Defines a formula for differentiating the operation with backward mode
      automatic differentiation (alias to the vjp function).

      This function is to be overridden by all subclasses.

      It must accept a context :attr:`ctx` as the first argument, followed by
      as many outputs as the :func:`forward` returned (None will be passed in
      for non tensor outputs of the forward function),
      and it should return as many tensors, as there were inputs to
      :func:`forward`. Each argument is the gradient w.r.t the given output,
      and each returned value should be the gradient w.r.t. the
      corresponding input. If an input is not a Tensor or is a Tensor not
      requiring grads, you can just pass None as a gradient for that input.

      The context can be used to retrieve tensors saved during the forward
      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple
      of booleans representing whether each input needs gradient. E.g.,
      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the
      first input to :func:`forward` needs gradient computated w.r.t. the
      output.



.. py:class:: FastApproxConv2dOp(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`

   torch.autograd.Function wrapper for High Througput model of ApproxConv2d operator
   uses fast model for forward pass and non-approximate gradients
   for backward pass (STE)

   .. py:method:: forward(ctx, x, w, model, kwargs)
      :staticmethod:

      Performs the operation.

      This function is to be overridden by all subclasses.

      It must accept a context ctx as the first argument, followed by any
      number of arguments (tensors or other types).

      The context can be used to store arbitrary data that can be then
      retrieved during the backward pass. Tensors should not be stored
      directly on `ctx` (though this is not currently enforced for
      backward compatibility). Instead, tensors should be saved either with
      :func:`ctx.save_for_backward` if they are intended to be used in
      ``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`
      if they are intended to be used for in ``jvp``.


   .. py:method:: backward(ctx, grad)
      :staticmethod:

      Defines a formula for differentiating the operation with backward mode
      automatic differentiation (alias to the vjp function).

      This function is to be overridden by all subclasses.

      It must accept a context :attr:`ctx` as the first argument, followed by
      as many outputs as the :func:`forward` returned (None will be passed in
      for non tensor outputs of the forward function),
      and it should return as many tensors, as there were inputs to
      :func:`forward`. Each argument is the gradient w.r.t the given output,
      and each returned value should be the gradient w.r.t. the
      corresponding input. If an input is not a Tensor or is a Tensor not
      requiring grads, you can just pass None as a gradient for that input.

      The context can be used to retrieve tensors saved during the forward
      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple
      of booleans representing whether each input needs gradient. E.g.,
      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the
      first input to :func:`forward` needs gradient computated w.r.t. the
      output.



.. py:class:: ApproxDWConv2dOp(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`

   torch.autograd.Function wrapper for GPU-accelerated Depthwise Conv

   .. py:method:: forward(ctx, x, w, lut, kwargs)
      :staticmethod:

      Performs the operation.

      This function is to be overridden by all subclasses.

      It must accept a context ctx as the first argument, followed by any
      number of arguments (tensors or other types).

      The context can be used to store arbitrary data that can be then
      retrieved during the backward pass. Tensors should not be stored
      directly on `ctx` (though this is not currently enforced for
      backward compatibility). Instead, tensors should be saved either with
      :func:`ctx.save_for_backward` if they are intended to be used in
      ``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`
      if they are intended to be used for in ``jvp``.


   .. py:method:: backward(ctx, grad)
      :staticmethod:

      Defines a formula for differentiating the operation with backward mode
      automatic differentiation (alias to the vjp function).

      This function is to be overridden by all subclasses.

      It must accept a context :attr:`ctx` as the first argument, followed by
      as many outputs as the :func:`forward` returned (None will be passed in
      for non tensor outputs of the forward function),
      and it should return as many tensors, as there were inputs to
      :func:`forward`. Each argument is the gradient w.r.t the given output,
      and each returned value should be the gradient w.r.t. the
      corresponding input. If an input is not a Tensor or is a Tensor not
      requiring grads, you can just pass None as a gradient for that input.

      The context can be used to retrieve tensors saved during the forward
      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple
      of booleans representing whether each input needs gradient. E.g.,
      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the
      first input to :func:`forward` needs gradient computated w.r.t. the
      output.



