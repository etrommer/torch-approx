:py:mod:`torchapprox.operators.linear`
======================================

.. py:module:: torchapprox.operators.linear


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   torchapprox.operators.linear.FastLinearOp




.. py:class:: FastLinearOp(*args, **kwargs)

   Bases: :py:obj:`torch.autograd.Function`

   torch.autograd.Function wrapper for Fast model.
   uses fast model for forward pass and non-approximate gradients
   for backward pass (STE)

   .. py:method:: forward(ctx, x: torch.Tensor, w: torch.Tensor, model: str)
      :staticmethod:

      Performs the operation.

      This function is to be overridden by all subclasses.

      It must accept a context ctx as the first argument, followed by any
      number of arguments (tensors or other types).

      The context can be used to store arbitrary data that can be then
      retrieved during the backward pass. Tensors should not be stored
      directly on `ctx` (though this is not currently enforced for
      backward compatibility). Instead, tensors should be saved either with
      :func:`ctx.save_for_backward` if they are intended to be used in
      ``backward`` (equivalently, ``vjp``) or :func:`ctx.save_for_forward`
      if they are intended to be used for in ``jvp``.


   .. py:method:: backward(ctx, grad)
      :staticmethod:

      Defines a formula for differentiating the operation with backward mode
      automatic differentiation (alias to the vjp function).

      This function is to be overridden by all subclasses.

      It must accept a context :attr:`ctx` as the first argument, followed by
      as many outputs as the :func:`forward` returned (None will be passed in
      for non tensor outputs of the forward function),
      and it should return as many tensors, as there were inputs to
      :func:`forward`. Each argument is the gradient w.r.t the given output,
      and each returned value should be the gradient w.r.t. the
      corresponding input. If an input is not a Tensor or is a Tensor not
      requiring grads, you can just pass None as a gradient for that input.

      The context can be used to retrieve tensors saved during the forward
      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple
      of booleans representing whether each input needs gradient. E.g.,
      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the
      first input to :func:`forward` needs gradient computated w.r.t. the
      output.



