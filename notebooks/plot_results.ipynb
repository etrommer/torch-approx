{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_files = fm.findSystemFonts(fontpaths=['/mnt/c/Windows/Fonts', '/mnt/c/Users/trommer/AppData/Local/Microsoft/Windows/Fonts'], fontext='otf')\n",
    "\n",
    "for font_file in font_files:\n",
    "    fm.fontManager.addfont(font_file)\n",
    "\n",
    "import matplotlib as mpl\n",
    "#mpl.rcParams['font.family'] = 'Neue Haas Grotesk Text Pro'\n",
    "# mpl.rcParams['font.family'] = 'Linux Biolinum'\n",
    "mpl.rcParams['font.family'] = 'CMU Sans Serif'\n",
    "\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "colors = sns.color_palette('Set2')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    benchmarks = data['benchmarks']\n",
    "\n",
    "    res = []\n",
    "    for b in benchmarks:\n",
    "        flat_dict = {**b['params'], **b['stats']}\n",
    "        res.append(flat_dict)\n",
    "\n",
    "    df = pd.DataFrame.from_records(res)\n",
    "    b_name = os.path.basename(path)\n",
    "    f_name, _ = os.path.splitext(b_name)\n",
    "    df['filename'] = f_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = read_results('benchmark_results/221215_conv_layer_torchapprox.json')\n",
    "tf = read_results('benchmark_results/221215_conv_layer_tfapprox.json')\n",
    "df = pd.concat([ta, tf], ignore_index=True)\n",
    "df.loc[df.bench_type.isna(), 'bench_type'] = 'tfapprox'\n",
    "df.bench_type = df.bench_type.apply(lambda s: s[0] if isinstance(s, list) else s)\n",
    "df.sort_values(['channels', 'bench_type'], inplace=True, ignore_index=True)\n",
    "df = df[~(df.bench_type.str.contains('mul12') | df.bench_type.str.contains('mul16'))]\n",
    "df = df[df.channels > 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5,2.2))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "# ax.yaxis.set_tick_params(which='minor', size=2, width=0.75)\n",
    "\n",
    "params = [\n",
    "    \n",
    "    ('mul8s', 'HTP, EvoApp. 8-Bit', 'o'),\n",
    "    ('lut','LUT, TorchApprox', 's'),\n",
    "    ('tfapprox', 'LUT, TFApprox', '^'),\n",
    "]\n",
    "for key, name, marker in params:\n",
    "    models = df[df.bench_type.str.contains(key)]\n",
    "    if key == 'mul8s':\n",
    "        means = models.groupby('channels')['median'].agg('mean')\n",
    "        stds = models.groupby('channels')['median'].agg('std')\n",
    "        x = np.unique(models.channels)\n",
    "        plt.errorbar(x, means.values * 1000, yerr=stds.values * 1000, label=name, capsize = 1.5, linestyle='--', ecolor=colors[0], elinewidth=0.8, markersize=2.0, markerfacecolor='white', marker=marker, markeredgecolor='black', barsabove=False, zorder=3)\n",
    "        # plt.errorbar(x, means.values * 1000, yerr=stds.values * 1000)\n",
    "    else:\n",
    "        plt.plot(models.channels, models['median'] * 1000, label=name, marker=marker, linestyle='--', markersize=5.0, markeredgecolor='black', markeredgewidth=0.8)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of Channels', fontsize=6)\n",
    "ax.set_ylabel('Median Execution Time [ms]', fontsize=6)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_position(('outward', 6))\n",
    "ax.spines['bottom'].set_position(('outward', 4))\n",
    "ax.spines['bottom'].set_bounds(2, 64)\n",
    "\n",
    "ax.set_xticks(np.unique(df.channels))\n",
    "ax.set_xticklabels(np.unique(df.channels), fontsize=7)\n",
    "ax.xaxis.set_tick_params(which='major', size=2, width=1)\n",
    "ax.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax.yaxis.set_tick_params(which='major', size=5, width=1, direction='out')\n",
    "ax.yaxis.set_tick_params(which='minor', size=2, width=0.75, direction='out')\n",
    "ax.yaxis.grid(linestyle='dotted')\n",
    "#ax.set_ylim(2e-1, 1e3)\n",
    "# ax.set_xlim(1.4,70)\n",
    "\n",
    "ax.legend(ncol=2, frameon=False, loc='upper left', bbox_to_anchor=(0.05, 1.40), fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.savefig('conv2d_benchmark.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "vals = df[df.bench_type == 'tfapprox']['median'] / df[df.bench_type.str.contains('mul8')].groupby('channels')['median'].agg('mean').values\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_names = {\n",
    "    'alexnet' : 'AlexNet',\n",
    "    'effcientnet_b0' : 'EfficientNetB0',\n",
    "    'mobilenet_v2' : 'MobileNetV2',\n",
    "    'resnet18' : 'ResNet18',\n",
    "    'resnet50' : 'ResNet50',\n",
    "    'vgg16' : 'VGG16',\n",
    "}\n",
    "df = read_results('benchmark_results/221205_networks.json')\n",
    "df.loc[df.bench_type.isna(), 'bench_type'] = 'adaPT'\n",
    "df.bench_type = df.bench_type.apply(lambda s: s[0] if isinstance(s, list) else s)\n",
    "df = df.loc[~df.bench_type.str.contains(\"accurate\")]\n",
    "df.bench_architecture = df.bench_architecture.map(pretty_names)\n",
    "df.sort_values(['bench_architecture', 'bench_type'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmean(df[df.bench_type == 'adaPT']['median'] / df[df.bench_type.str.contains('lut')].groupby('bench_architecture')['median'].agg('mean').values))\n",
    "print(gmean(df[df.bench_type == 'adaPT']['median'] / df[df.bench_type.str.contains('mul8s')].groupby('bench_architecture')['median'].agg('mean').values))\n",
    "print(gmean(df[df.bench_type == 'lut']['median'] / df[df.bench_type.str.contains('mul8s')].groupby('bench_architecture')['median'].agg('mean').values))\n",
    "print((df[df.bench_type == 'lut']['median'] / df[df.bench_type.str.contains('baseline')].groupby('bench_architecture')['median'].agg('mean').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    # ('mul8s_1L2D', 'HTP Model 1L2D', '///'),\n",
    "    ('mul8s', 'HTP, 8-Bit', '\\\\\\\\\\\\\\\\', 0),\n",
    "    ('mul12s', 'HTP, 12-Bit', '////', 1),\n",
    "    ('mul16s', 'HTP, 16-Bit', 'xxxx', 2),\n",
    "    ('lut', 'LUT, TorchApprox', '////', 1),\n",
    "    ('adaPT', 'LUT, adaPT', '...', 2),\n",
    "    ('baseline', 'Quant. only', None, 3),\n",
    "]\n",
    "\n",
    "width = 0.85/len(params)\n",
    "\n",
    "fig = plt.figure(figsize=(3.5,2.6))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "for i, (key, label, hatch, c_idx) in enumerate(params):\n",
    "    ans = df[df.bench_type.str.contains(key)]\n",
    "    x = np.arange(len(np.unique(ans.bench_architecture))) + i*width\n",
    "    if key == 'baseline':\n",
    "        ax.bar(x, height=ans['median'].values * 1000, width=width, align='edge', edgecolor='grey', color = 'lightgrey', label=label, zorder=1)\n",
    "    elif 'mul' in key:\n",
    "        palette = sns.dark_palette(colors[0], 5, reverse=True)\n",
    "        means = ans.groupby('bench_architecture')['median'].agg('mean')\n",
    "        stds = ans.groupby('bench_architecture')['median'].agg('std')\n",
    "        ax.bar(x, height=means.values * 1000, yerr=stds.values * 1000, capsize=1.2, error_kw = {'lw':0.6, 'mew': 0.6}, width=width, align='edge', edgecolor='black', color = palette[c_idx], label=label, zorder=2)\n",
    "    else:\n",
    "        ax.bar(x, height=ans['median'].values * 1000, width=width, align='edge', edgecolor='black', color = colors[c_idx], hatch=hatch, label=label, zorder=2)\n",
    "\n",
    "labels = np.unique(df.bench_architecture)\n",
    "ax.xaxis.set_ticks(np.arange(len(labels)) + len(params)/2 * width, labels, rotation=25, size=7)\n",
    "ax.xaxis.set_tick_params(bottom=False)\n",
    "# ax.set_xlim(-0.4, len(np.unique(df.bench_architecture))+0.2)\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Median Execution Time [ms]', fontsize=7)\n",
    "# ax.set_ylim(5, 1e5)\n",
    "ax.yaxis.set_major_locator(ticker.LogLocator(numticks=5))\n",
    "ax.yaxis.set_tick_params(which='major', size=4, width=1, direction='out')\n",
    "ax.yaxis.set_tick_params(which='minor', size=2, width=0.75, direction='out')\n",
    "ax.yaxis.grid(which='major', linestyle='dotted')\n",
    "\n",
    "ax.spines['left'].set_position(('outward', 8))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend(ncol=3, frameon=False, loc='upper left', bbox_to_anchor=(-0.18, 1.40), fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_benchmark.pdf', bbox_inches='tight')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_accuracy(path, target_metric):\n",
    "    df = pd.read_csv(path).loc[:,['Duration', 'Name', 'epochs',target_metric]]\n",
    "    df = df.drop(df.index[df.Name.str.contains('QAT Model') | df.Name.str.contains('Baseline Model')])\n",
    "\n",
    "    df.loc[df.Name.str.contains('Noise'),'type'] = 'noise'\n",
    "    df.loc[df.Name.str.contains('Behavioral'),'type'] = 'behavioral'\n",
    "    df.loc[df.Name.str.contains('Baseline'),'type'] = 'baseline'\n",
    "    df.loc[df.Name.str.contains('HTP'),'type'] = 'htp'\n",
    "\n",
    "\n",
    "    def time_to_float(s: str) -> float:\n",
    "        t = float(re.findall(r'\\d*.\\d{1,2}', s)[0])\n",
    "        if 'min' in s:\n",
    "            t *= 60\n",
    "        return t\n",
    "\n",
    "    def mul_name(s: str) -> str:\n",
    "        return re.findall(r'mul8s_[\\w\\d]{4}', s)[0]\n",
    "        \n",
    "    df.Duration = df.Duration.map(time_to_float)\n",
    "    df['Multiplier'] = df.Name.map(mul_name)\n",
    "\n",
    "    for m in np.unique(df.Multiplier):\n",
    "        bl = df.loc[(df.Multiplier == m) & (df.type=='behavioral'), target_metric].values[0]\n",
    "        df.loc[df.Multiplier == m, 'Deviation'] = df.loc[df.Multiplier == m, target_metric] - bl\n",
    "\n",
    "        bl = df.loc[(df.Multiplier == m) & (df.type=='baseline'), 'Duration'].values[0]\n",
    "        df.loc[df.Multiplier == m, 'Overhead'] = (df.loc[df.Multiplier == m, 'Duration'] / bl) -1.0\n",
    "    df.Deviation = df.Deviation.abs()\n",
    "\n",
    "    def print_result(df: pd.DataFrame, key: str, scale=1.0):\n",
    "        mean = (df.groupby('type')[key].agg('mean') * scale).round(2)\n",
    "        std = (df.groupby('type')[key].agg('std') * scale).round(2)\n",
    "\n",
    "        print(\"\\n\" + key.upper())\n",
    "        for m, s, l in zip(mean, std, mean.index):\n",
    "            print(f\"{l}: {m} $\\pm$ {s}\")\n",
    "    \n",
    "    \n",
    "    print_result(df, target_metric, 100.0)\n",
    "    print_result(df, 'Overhead', 100.0)\n",
    "    print_result(df, 'Deviation', 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_accuracy('accuracy_results/230105_accuracy_lenet5_mnist.csv', 'test_acc_top1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_accuracy('accuracy_results/230105_accuracy_resnet8_cifar.csv', 'test_acc_top1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_accuracy('accuracy_results/230109_accuracy_vgg16_tinyimagenet.csv', 'test_acc_top1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_dict = {}\n",
    "\n",
    "for bw in [8, 12, 16]:\n",
    "    fname = f\"htp_params_mul{bw}s.json\"\n",
    "    with open(fname, 'r') as fh:\n",
    "        data = json.load(fh)\n",
    "    for mul in data:\n",
    "        d1 = dict(('poly_' + k, v) for k,v in mul['baseline_metrics'].items()) \n",
    "        d1.update(dict(('htp_' + k, v) for k,v in mul['htp_metrics'].items()))\n",
    "        d1['coefficients'] = len(mul['htp_params']['coefficients'])\n",
    "        flat_dict[mul['name']] = d1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(flat_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(df,2).loc[:,['poly_mre', 'htp_mre']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "21ca7a3a434aac1b7012d03dfba7994e9dea88e405d4042a505b1670b651b470"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
