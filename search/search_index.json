{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorchApprox Documentation","text":"<p>TorchApprox is a PyTorch extension that allows for using non-standard multiplication functions in PyTorch. This makes it easy to study the effects of Approximate Hardware Multipliers on Neural Network tasks. Currently <code>torch.nn.Linear</code> and <code>torch.nn.Conv2d</code> subclasses are implemented. Approximate Multipliers can help lower the arithmetic footprint of NN hardware with only minor reductions in accuracy. By default, integer multipliers with up to 8x8-Bit inputs are supported. These bitwidths are the standard numerical format when deploying neural networks on constrained hardware.</p> <p>The custom product function for the 8x8-Bit input space is expected to be pre-computed and saved as a Numpy array.</p> <p>Quantization of intermediate results during training uses the PyTorch Quantization API. </p> <p>TorchApprox is a companion package to agn-approx. TorchApprox implements low-level primitives, operators and approximate layers, while agn-approx provides high-level neural network experimental setups.</p>"},{"location":"changelog/","title":"Changelog","text":"<p><code>{include} ../CHANGELOG.md</code></p>"},{"location":"conduct/","title":"Code of Conduct","text":""},{"location":"conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant homepage, version 1.4.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>You can never have enough documentation! Please feel free to contribute to any part of the documentation, such as the official docs, docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>torchapprox</code> for local development.</p> <ol> <li>Download a copy of <code>torchapprox</code> locally.</li> <li> <p>Install <code>torchapprox</code> using <code>poetry</code>:</p> <pre><code>$ poetry install\n</code></pre> </li> <li> <p>Use <code>git</code> (or similar) to create a branch for local development and make your changes:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>When you're done making changes, check that your changes conform to any code formatting requirements and pass any tests.</p> </li> <li> <p>Commit your changes and open a pull request.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include additional tests if appropriate.</li> <li>If the pull request adds functionality, the docs should be updated.</li> <li>The pull request should work for all currently supported operating systems and versions of Python.</li> </ol>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that the <code>torchapprox</code> project is released with a Code of Conduct. By contributing to this project you agree to abide by its terms.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#dependencies","title":"Dependencies","text":"<p>TorchApprox relies on the ninja build system and OpenMP in order to JIT-compile kernels. Make sure they are available on your system.</p>"},{"location":"install/#ubuntu","title":"Ubuntu","text":"<pre><code>$ sudo apt install libomp-dev ninja-build\n</code></pre>"},{"location":"install/#pip-installation","title":"Pip Installation","text":"<pre><code>$ pip install git+ssh://github.com/etrommer/torch-approx\n</code></pre>"},{"location":"install/#development","title":"Development","text":"<p>If you want to help actively develop TorchApprox, clone TorchApprox from Github and make sure to install additional dependencies as well as pre-commit hooks: <pre><code>git clone git@github.com:etrommer/torch-approx.git\ncd torch-approx\npoetry install --with dev extras\npoetry run pre-commit install\n</code></pre></p>"},{"location":"install/#unit-tests","title":"Unit Tests","text":"<p>TorchApprox uses pytest for unit testing. Unit tests can be run with: <pre><code>poetry run pytest test\n</code></pre> If a CUDA device is detected, tests will be run on CPU and GPU. Otherwise, tests run on CPU only.</p>"},{"location":"install/#micro-benchmarking","title":"Micro-Benchmarking","text":"<p>TorchApprox uses the pytest-benchmark plugin to perform benchmarking of low-level kernels. Benchmarking can be run with: <pre><code>poetry run pytest benchmarks\n</code></pre></p>"},{"location":"mwe/","title":"TorchApprox QuickStart","text":""},{"location":"mwe/#usage","title":"Usage","text":"<p>Let's assume that you have a vanilla PyTorch CNN model on which we want to try approximate multiplications: <pre><code>import torchvision.models as models\nmodel = models.mobilenet_v2()\n</code></pre> First, we need to apply quantization to the model to have a meaningful conversion from floating-point weights and activations to integers. Integers are required since they are commonly used for deployment and are the input format for approximate multipliers. Due to an idiosyncracy in how PyTorch's quantization is implemented, we first need to wrap each layer that we want to quantize in a <code>torchapprox.layers.ApproxWrapper</code> instance: <pre><code>from torchapprox.utils import wrap_quantizable\n\n# Wrap Linear and Conv2d layer instances\nwrap_quantizable(model)\n</code></pre> after that, the model can be converted using the regular <code>torch.ao.quantziation.prepare_qat</code> function.We supply a custom layer mapping to make sure that layers are replaced with TorchApprox' quantized layer implementations, rather than Pytorch's. <pre><code>import torch.ao.quantization as quant\nimport torchapprox.layers as tal\n\n# Convert Linear and Conv2d layers to their quantized equivalents\nquant.preprare_qat(model, tal.layer_mapping_dict(), inplace=True)\n</code></pre> It is recommended to first run a few epochs of Quantization-aware training with accurate multiplications to calibrate weights and quantization parameters. This is done with a regular Pytorch training loop on the converted model. After the quantization parameters have been calibrated successfully, the model can be switched into approximate multiplication mode. Additionally, we need to supply a Lookup Table of pre-computed approximate multiplication results. The lookup table is a 2D Numpy array of size 256x256. <pre><code>import numpy as np\nfrom torchapprox.utils import get_approx_modules\n\n# We simply use the result of an accurate multiplication as an example.\n# Adjust the contents of `lut` to suit your needs.\nx = y = np.arange(256)\nxx, yy = np.meshgrid(x, y)\nlut = xx*yy\n\nfor _, m in get_approx_modules(model):\n    m.inference_mode = tal.InferenceMode.APPROXIMATE\n    m.approx_op.lut = lut\n</code></pre> The next training loop will now implement multiplications <code>y = x * w</code> in all replaced layers as a lookup operation <code>y = lut[x][w]</code>.</p> <p>The companion project agn-approx wraps these primitives in a high-level API using pytorch-lightning and can be used as a reference or starting point for a less verbose implemnetation of experiments.</p>"},{"location":"mwe/#lookup-table-ordering","title":"Lookup Table Ordering","text":"<p>For unsigned multipliers, both axis of the LUT need to be ordered numerically: <pre><code>x = y = [0, 1, 2, ..., 254, 255]\n</code></pre> For signed multipliers, the axes  of the lookup table are not ordered numerically, but by the numerical order of the unsigned twos-complement equivalent of each index, i.e.: <pre><code>x = y = [0, 1, 2, ... 126, 127, -128, -127, ...-2, -1]\n</code></pre></p>"},{"location":"reference/layers/","title":"Layers","text":"<p>Approximate Layer implementations</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d","title":"<code>ApproxConv2d</code>","text":"<p>             Bases: <code>ApproxLayer</code>, <code>Conv2d</code></p> <p>Approximate 2D Convolution layer implementation</p> Source code in <code>src/torchapprox/layers/approx_conv2d.py</code> <pre><code>class ApproxConv2d(ApproxLayer, QATConv2d):\n    \"\"\"\n    Approximate 2D Convolution layer implementation\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: Union[str, _size_2_t] = 0,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"zeros\",\n        qconfig=None,\n        device=None,\n        dtype=None,\n    ) -&gt; None:\n        QATConv2d.__init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n            qconfig,\n            device,\n            dtype,\n        )\n        ApproxLayer.__init__(self)\n        assert (\n            padding_mode == \"zeros\"\n        ), f\"Unsupported padding_mode {padding_mode}, only zero-padding is supported\"\n        self._opcount = None\n        self.to(self.weight.device)\n\n    @staticmethod\n    def from_super(cls_instance: torch.nn.Conv2d):\n        \"\"\"\n        Alias for from_conv2d\n        \"\"\"\n        return ApproxConv2d.from_conv2d(cls_instance)\n\n    @staticmethod\n    def from_conv2d(conv2d: torch.nn.Conv2d):\n        \"\"\"\n        Construct ApproxConv2d from torch.nn.Conv2d layer\n        \"\"\"\n        has_bias = conv2d.bias is not None\n        approx_instance = ApproxConv2d(\n            conv2d.in_channels,\n            conv2d.out_channels,\n            conv2d.kernel_size,\n            stride=conv2d.stride,\n            padding=conv2d.padding,\n            dilation=conv2d.dilation,\n            groups=conv2d.groups,\n            bias=has_bias,\n            padding_mode=conv2d.padding_mode,\n        )\n\n        with torch.no_grad():\n            approx_instance.weight = conv2d.weight\n            if has_bias:\n                approx_instance.bias = conv2d.bias\n\n        return approx_instance\n\n    def output_dims(self, x):\n        \"\"\"\n        Output width and height\n        \"\"\"\n\n        def dim(idx):\n            # Copied from\n            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n            return math.floor(\n                (\n                    x.size(idx + 2)\n                    + 2 * self.padding[idx]\n                    - self.dilation[idx] * (self.kernel_size[idx] - 1)\n                    - 1\n                )\n                / self.stride[idx]\n                + 1\n            )\n\n        return (dim(0), dim(1))\n\n    @property\n    def opcount(self) -&gt; int:\n        if self._opcount is None:\n            raise ValueError(\n                \"Conv layer Opcount not populated. Run forward pass first.\"\n            )\n        return self._opcount\n\n    @property\n    def fan_in(self) -&gt; int:\n        \"\"\"\n        Number of incoming connection for a single neuron\n        \"\"\"\n        return self.in_channels * math.prod(self.kernel_size)\n\n    @property\n    def conv_args(self) -&gt; Conv2dArgs:\n        \"\"\"\n        Wrap layer configuration in dataclass for more convenient passing around\n        \"\"\"\n        args = Conv2dArgs(\n            in_channels=self.in_channels,\n            out_channels=self.out_channels,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups,\n        )\n        return args\n\n    def quant_fwd(self, x_q, w_q):\n        y = torch.nn.functional.conv2d(\n            x_q,\n            w_q,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups,\n        )\n        return y\n\n    def approx_fwd(self, x_q, w_q, quant_params: QuantizationParameters):\n        y = ApproxConv2dOp.apply(\n            x_q,\n            w_q,\n            quant_params,\n            self.conv_args,\n            self.htp_model,\n            self.output_dims(x_q),\n            self.approx_op.lut,\n        )\n\n        return y\n\n    # pylint: disable=arguments-renamed\n    def forward(\n        self,\n        x_q: torch.Tensor,\n        x_scale: Optional[torch.Tensor] = None,\n        x_zero_point: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        # calculate opcount for this layer using the input tensor size on first forward pass\n        if self._opcount is None:\n            self._opcount = int(\n                math.prod(self.kernel_size)\n                * math.prod(self.output_dims(x_q))\n                * (self.in_channels / self.groups)\n                * self.out_channels\n            )\n\n        # Reshape bias tensor to make it broadcastable\n        bias = None if self.bias is None else self.bias[:, None, None]\n        return ApproxLayer.forward(self, x_q, x_scale, x_zero_point, bias)\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d.conv_args","title":"<code>conv_args: Conv2dArgs</code>  <code>property</code>","text":"<p>Wrap layer configuration in dataclass for more convenient passing around</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d.fan_in","title":"<code>fan_in: int</code>  <code>property</code>","text":"<p>Number of incoming connection for a single neuron</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d.from_conv2d","title":"<code>from_conv2d(conv2d)</code>  <code>staticmethod</code>","text":"<p>Construct ApproxConv2d from torch.nn.Conv2d layer</p> Source code in <code>src/torchapprox/layers/approx_conv2d.py</code> <pre><code>@staticmethod\ndef from_conv2d(conv2d: torch.nn.Conv2d):\n    \"\"\"\n    Construct ApproxConv2d from torch.nn.Conv2d layer\n    \"\"\"\n    has_bias = conv2d.bias is not None\n    approx_instance = ApproxConv2d(\n        conv2d.in_channels,\n        conv2d.out_channels,\n        conv2d.kernel_size,\n        stride=conv2d.stride,\n        padding=conv2d.padding,\n        dilation=conv2d.dilation,\n        groups=conv2d.groups,\n        bias=has_bias,\n        padding_mode=conv2d.padding_mode,\n    )\n\n    with torch.no_grad():\n        approx_instance.weight = conv2d.weight\n        if has_bias:\n            approx_instance.bias = conv2d.bias\n\n    return approx_instance\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d.from_super","title":"<code>from_super(cls_instance)</code>  <code>staticmethod</code>","text":"<p>Alias for from_conv2d</p> Source code in <code>src/torchapprox/layers/approx_conv2d.py</code> <pre><code>@staticmethod\ndef from_super(cls_instance: torch.nn.Conv2d):\n    \"\"\"\n    Alias for from_conv2d\n    \"\"\"\n    return ApproxConv2d.from_conv2d(cls_instance)\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxConv2d.output_dims","title":"<code>output_dims(x)</code>","text":"<p>Output width and height</p> Source code in <code>src/torchapprox/layers/approx_conv2d.py</code> <pre><code>def output_dims(self, x):\n    \"\"\"\n    Output width and height\n    \"\"\"\n\n    def dim(idx):\n        # Copied from\n        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        return math.floor(\n            (\n                x.size(idx + 2)\n                + 2 * self.padding[idx]\n                - self.dilation[idx] * (self.kernel_size[idx] - 1)\n                - 1\n            )\n            / self.stride[idx]\n            + 1\n        )\n\n    return (dim(0), dim(1))\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer","title":"<code>ApproxLayer</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Derivable Abstract Base Class for implementing Approximate Neural Network layers</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>class ApproxLayer(ABC):\n    \"\"\"\n    Derivable Abstract Base Class for implementing Approximate Neural Network layers\n    \"\"\"\n\n    def __init__(self, qconfig: Optional[tq.QConfig] = None):\n        self.approx_op: LUTGeMM = LUTGeMM()\n        self.inference_mode: InferenceMode = InferenceMode.QUANTIZED\n        self.htp_model: Optional[Callable] = None\n\n        self._stdev: torch.Tensor = torch.tensor([0.0])\n        self._mean: torch.Tensor = torch.tensor([0.0])\n\n    @staticmethod\n    def default_qconfig() -&gt; tq.QConfig:\n        act_qconfig = tq.FakeQuantize.with_args(\n            observer=tq.HistogramObserver,\n            dtype=torch.quint8,\n            qscheme=torch.per_tensor_affine,\n            quant_min=0,\n            quant_max=127,\n        )\n        weight_qconfig = tq.FakeQuantize.with_args(\n            observer=tq.HistogramObserver,\n            dtype=torch.qint8,\n            qscheme=torch.per_tensor_symmetric,\n            quant_min=-128,\n            quant_max=127,\n        )\n        return tq.QConfig(activation=act_qconfig, weight=weight_qconfig)\n\n    @property\n    def stdev(self) -&gt; float:\n        \"\"\"\n        Perturbation Error Relative Standard Deviation\n\n        Returns:\n            Currently configured perturbation standard deviation\n        \"\"\"\n        return self._stdev.item()\n\n    @stdev.setter\n    def stdev(self, val: float):\n        self._stdev = torch.tensor([val], device=self.weight.device)  # type: ignore\n\n    @property\n    def mean(self) -&gt; float:\n        \"\"\"\n        Perturbation Error mean\n\n        Returns:\n            Currently configured perturbation mean\n        \"\"\"\n        return self._mean.item()\n\n    @mean.setter\n    def mean(self, val: float):\n        self._mean = torch.tensor([val], device=self.weight.device)  # type: ignore\n\n    @property\n    @abstractmethod\n    def fan_in(self) -&gt; int:\n        \"\"\"\n        Number of incoming connections for a neuron in this layer\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def opcount(self) -&gt; int:\n        \"\"\"\n        Number of multiplications for a single\n        forward pass of this layer\n        \"\"\"\n\n    @abstractmethod\n    def quant_fwd(\n        self, x: torch.FloatTensor, w: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        \"\"\"Quantized Forward Pass\n        Performs the layer operation with an additional pass through the\n        currently configured quantizer.\n\n        `x_q and w_q are expected to be **fake-quantized** tensors, i.e. floats that are\n        discretized to a set of values, but not converted to actual their integer\n        representation.\n\n        Args:\n            x_q: Fake-quantized activations\n            w_q: Fake-quantized weights\n\n        Returns:\n            Layer output\n        \"\"\"\n\n    @abstractmethod\n    def approx_fwd(\n        self,\n        x: torch.CharTensor,\n        w: torch.CharTensor,\n        quant_params: QuantizationParameters,\n    ):\n        \"\"\"Approximate Product Forward Pass\n        Performs the layer operation using the currently configured\n        approximate product Lookup Table.\n\n        Args:\n            x: Layer input\n\n        Returns:\n            Layer output\n        \"\"\"\n\n    @no_type_check\n    def noise_fwd(\n        self, x_q: torch.FloatTensor, w_q: torch.FloatTensor\n    ) -&gt; torch.FloatTensor:\n        \"\"\"Quantized Forward Pass that is perturbed\n        with Gaussian Noise\n\n        The standard deviation of the additive noise\n        is derived from the `stdev`parameter and scaled\n        with the standard deviation of the current batch\n\n        Args:\n            x: Layer input\n\n        Returns:\n            Layer output\n        \"\"\"\n        y = self.quant_fwd(x_q, w_q)\n        if self.training:\n            noise = torch.randn_like(y) * torch.std(y) * self.stdev + self.mean\n            y = y + noise\n        return y\n\n    @no_type_check\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_scale: Optional[torch.Tensor] = None,\n        x_zero_point: Optional[torch.Tensor] = None,\n        bias: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass with currently selected mode applied\n\n        Args:\n            x: Layer input\n\n        Returns:\n            Layer output\n        \"\"\"\n        assert hasattr(\n            self, \"weight_fake_quant\"\n        ), \"QAT nodes not replaced. Run `prepare_qat` first.\"\n\n        w = self.weight_fake_quant(self.weight)\n        if self.inference_mode == InferenceMode.NOISE:\n            y = self.noise_fwd(x, w)\n        elif self.inference_mode == InferenceMode.APPROXIMATE:\n            assert (x_scale is not None) and (\n                x_zero_point is not None\n            ), \"Received no activation quantization information during approximate forward pass\"\n            assert (\n                len(x_scale) == 1 and len(x_zero_point) == 1\n            ), \"Per-channel quantization only supported for weights\"\n            quant_params = QuantizationParameters(\n                x_scale,\n                x_zero_point,\n                self.weight_fake_quant.scale,\n                self.weight_fake_quant.zero_point,\n            )\n            y = self.approx_fwd(x, w, quant_params)\n        else:\n            y = self.quant_fwd(x, w)\n\n        if bias is not None:\n            y = y + bias\n        elif self.bias is not None:\n            y = y + self.bias\n\n        return y\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.fan_in","title":"<code>fan_in: int</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of incoming connections for a neuron in this layer</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.mean","title":"<code>mean: float</code>  <code>property</code> <code>writable</code>","text":"<p>Perturbation Error mean</p> <p>Returns:</p> Type Description <code>float</code> <p>Currently configured perturbation mean</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.opcount","title":"<code>opcount: int</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of multiplications for a single forward pass of this layer</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.stdev","title":"<code>stdev: float</code>  <code>property</code> <code>writable</code>","text":"<p>Perturbation Error Relative Standard Deviation</p> <p>Returns:</p> Type Description <code>float</code> <p>Currently configured perturbation standard deviation</p>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.approx_fwd","title":"<code>approx_fwd(x, w, quant_params)</code>  <code>abstractmethod</code>","text":"<p>Approximate Product Forward Pass Performs the layer operation using the currently configured approximate product Lookup Table.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>CharTensor</code> <p>Layer input</p> required <p>Returns:</p> Type Description <p>Layer output</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>@abstractmethod\ndef approx_fwd(\n    self,\n    x: torch.CharTensor,\n    w: torch.CharTensor,\n    quant_params: QuantizationParameters,\n):\n    \"\"\"Approximate Product Forward Pass\n    Performs the layer operation using the currently configured\n    approximate product Lookup Table.\n\n    Args:\n        x: Layer input\n\n    Returns:\n        Layer output\n    \"\"\"\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.forward","title":"<code>forward(x, x_scale=None, x_zero_point=None, bias=None)</code>","text":"<p>Forward pass with currently selected mode applied</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Layer input</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Layer output</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>@no_type_check\ndef forward(\n    self,\n    x: torch.Tensor,\n    x_scale: Optional[torch.Tensor] = None,\n    x_zero_point: Optional[torch.Tensor] = None,\n    bias: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass with currently selected mode applied\n\n    Args:\n        x: Layer input\n\n    Returns:\n        Layer output\n    \"\"\"\n    assert hasattr(\n        self, \"weight_fake_quant\"\n    ), \"QAT nodes not replaced. Run `prepare_qat` first.\"\n\n    w = self.weight_fake_quant(self.weight)\n    if self.inference_mode == InferenceMode.NOISE:\n        y = self.noise_fwd(x, w)\n    elif self.inference_mode == InferenceMode.APPROXIMATE:\n        assert (x_scale is not None) and (\n            x_zero_point is not None\n        ), \"Received no activation quantization information during approximate forward pass\"\n        assert (\n            len(x_scale) == 1 and len(x_zero_point) == 1\n        ), \"Per-channel quantization only supported for weights\"\n        quant_params = QuantizationParameters(\n            x_scale,\n            x_zero_point,\n            self.weight_fake_quant.scale,\n            self.weight_fake_quant.zero_point,\n        )\n        y = self.approx_fwd(x, w, quant_params)\n    else:\n        y = self.quant_fwd(x, w)\n\n    if bias is not None:\n        y = y + bias\n    elif self.bias is not None:\n        y = y + self.bias\n\n    return y\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.noise_fwd","title":"<code>noise_fwd(x_q, w_q)</code>","text":"<p>Quantized Forward Pass that is perturbed with Gaussian Noise</p> <p>The standard deviation of the additive noise is derived from the <code>stdev</code>parameter and scaled with the standard deviation of the current batch</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Layer input</p> required <p>Returns:</p> Type Description <code>FloatTensor</code> <p>Layer output</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>@no_type_check\ndef noise_fwd(\n    self, x_q: torch.FloatTensor, w_q: torch.FloatTensor\n) -&gt; torch.FloatTensor:\n    \"\"\"Quantized Forward Pass that is perturbed\n    with Gaussian Noise\n\n    The standard deviation of the additive noise\n    is derived from the `stdev`parameter and scaled\n    with the standard deviation of the current batch\n\n    Args:\n        x: Layer input\n\n    Returns:\n        Layer output\n    \"\"\"\n    y = self.quant_fwd(x_q, w_q)\n    if self.training:\n        noise = torch.randn_like(y) * torch.std(y) * self.stdev + self.mean\n        y = y + noise\n    return y\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLayer.quant_fwd","title":"<code>quant_fwd(x, w)</code>  <code>abstractmethod</code>","text":"<p>Quantized Forward Pass Performs the layer operation with an additional pass through the currently configured quantizer.</p> <p>`x_q and w_q are expected to be fake-quantized tensors, i.e. floats that are discretized to a set of values, but not converted to actual their integer representation.</p> <p>Parameters:</p> Name Type Description Default <code>x_q</code> <p>Fake-quantized activations</p> required <code>w_q</code> <p>Fake-quantized weights</p> required <p>Returns:</p> Type Description <code>FloatTensor</code> <p>Layer output</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>@abstractmethod\ndef quant_fwd(\n    self, x: torch.FloatTensor, w: torch.FloatTensor\n) -&gt; torch.FloatTensor:\n    \"\"\"Quantized Forward Pass\n    Performs the layer operation with an additional pass through the\n    currently configured quantizer.\n\n    `x_q and w_q are expected to be **fake-quantized** tensors, i.e. floats that are\n    discretized to a set of values, but not converted to actual their integer\n    representation.\n\n    Args:\n        x_q: Fake-quantized activations\n        w_q: Fake-quantized weights\n\n    Returns:\n        Layer output\n    \"\"\"\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxLinear","title":"<code>ApproxLinear</code>","text":"<p>             Bases: <code>ApproxLayer</code>, <code>Linear</code></p> <p>Approximate Linear Layer implementation</p> Source code in <code>src/torchapprox/layers/approx_linear.py</code> <pre><code>class ApproxLinear(ApproxLayer, QATLinear):\n    \"\"\"\n    Approximate Linear Layer implementation\n    \"\"\"\n\n    _FLOAT_MODULE = nn.Linear\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        bias=True,\n        qconfig=None,\n        device=None,\n        dtype=None,\n    ):\n        QATLinear.__init__(\n            self, in_features, out_features, bias, qconfig, device, dtype\n        )\n        ApproxLayer.__init__(self)\n        self._opcount = torch.tensor(self.in_features * self.out_features).float()\n        self.to(self.weight.device)\n\n    @property\n    def fan_in(self) -&gt; int:\n        return int(self.in_features)\n\n    @property\n    def opcount(self) -&gt; int:\n        return int(self._opcount)\n\n    def quant_fwd(self, x, w):\n        return torch.nn.functional.linear(x, w)\n\n    def approx_fwd(self, x, w, quant_params: QuantizationParameters):\n        y = self.approx_op(x, w, quant_params, self.htp_model)\n        return y\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxWrapper","title":"<code>ApproxWrapper</code>","text":"<p>             Bases: <code>Module</code></p> <p>Wrapper for adding quant/dequant stubs to a linear layer in a model.</p> <p>PyTorch provides the option to wrap modules in quantizers automatically, however a custom module is necessary so that we can forward the activation quantization scale and zero point to the approximate layer in the forward function.</p> <p>The wrapped instance of <code>torch.nn.Module</code> is meant to be replaced with an instance of <code>torchapprox.layers.ApproxLayer</code> in a separate call to <code>torch.ao.quantization.prepare()</code> after it has been wrapped here.</p> Source code in <code>src/torchapprox/layers/approx_wrapper.py</code> <pre><code>class ApproxWrapper(torch.nn.Module):\n    \"\"\"\n    Wrapper for adding quant/dequant stubs to a linear layer in a model.\n\n    PyTorch provides the option to wrap modules in quantizers automatically,\n    however a custom module is necessary so that we can forward the activation\n    quantization scale and zero point to the approximate layer in the forward function.\n\n    The wrapped instance of `torch.nn.Module` is meant to be replaced with an instance of\n    `torchapprox.layers.ApproxLayer` in a separate call to\n    `torch.ao.quantization.prepare()` after it has been wrapped here.\n    \"\"\"\n\n    def __init__(\n        self,\n        wrapped: Union[torch.nn.Linear, torch.nn.Conv2d],\n        qconfig: Optional[tq.QConfig] = None,\n    ):\n        \"\"\"\n        Wrap a torch.nn.linear layer with quantization stubs\n\n        Args:\n            wrapped: the layer to be wrapped\n            qconfig: Quantization configuration. Defaults to None.\n        \"\"\"\n        torch.nn.Module.__init__(self)\n        self.quant_stub = tq.QuantStub()\n        self.dequant_stub = tq.DeQuantStub()\n\n        assert isinstance(wrapped, torch.nn.Linear) or isinstance(\n            wrapped, torch.nn.Conv2d\n        ), f\"Received unknown layer type for wrapping: {type(wrapped)}\"\n        self.wrapped = wrapped\n\n        if not qconfig:\n            qconfig = ApproxLayer.default_qconfig()\n\n        self.qconfig = qconfig\n\n    @staticmethod\n    def from_float(wrapped):\n        return ApproxWrapper(wrapped)\n\n    def forward(self, x):\n        x_q = self.quant_stub(x)\n        x_scale = getattr(self.quant_stub.activation_post_process, \"scale\", None)\n        x_zero_point = getattr(\n            self.quant_stub.activation_post_process, \"zero_point\", None\n        )\n        y_q = self.wrapped(x_q, x_scale, x_zero_point)\n        y = self.dequant_stub(y_q)\n        return y\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.ApproxWrapper.__init__","title":"<code>__init__(wrapped, qconfig=None)</code>","text":"<p>Wrap a torch.nn.linear layer with quantization stubs</p> <p>Parameters:</p> Name Type Description Default <code>wrapped</code> <code>Union[Linear, Conv2d]</code> <p>the layer to be wrapped</p> required <code>qconfig</code> <code>Optional[QConfig]</code> <p>Quantization configuration. Defaults to None.</p> <code>None</code> Source code in <code>src/torchapprox/layers/approx_wrapper.py</code> <pre><code>def __init__(\n    self,\n    wrapped: Union[torch.nn.Linear, torch.nn.Conv2d],\n    qconfig: Optional[tq.QConfig] = None,\n):\n    \"\"\"\n    Wrap a torch.nn.linear layer with quantization stubs\n\n    Args:\n        wrapped: the layer to be wrapped\n        qconfig: Quantization configuration. Defaults to None.\n    \"\"\"\n    torch.nn.Module.__init__(self)\n    self.quant_stub = tq.QuantStub()\n    self.dequant_stub = tq.DeQuantStub()\n\n    assert isinstance(wrapped, torch.nn.Linear) or isinstance(\n        wrapped, torch.nn.Conv2d\n    ), f\"Received unknown layer type for wrapping: {type(wrapped)}\"\n    self.wrapped = wrapped\n\n    if not qconfig:\n        qconfig = ApproxLayer.default_qconfig()\n\n    self.qconfig = qconfig\n</code></pre>"},{"location":"reference/layers/#torchapprox.layers.InferenceMode","title":"<code>InferenceMode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Layer inference mode. Can be any of: - <code>quant</code>: Run inference using the layer's quantizer - <code>approx</code>: Run inference using approximate product LUT - <code>noise</code>: Run inference that is perturbed with additive Gaussian noise</p> Source code in <code>src/torchapprox/layers/approx_layer.py</code> <pre><code>class InferenceMode(enum.Enum):\n    \"\"\"\n    Layer inference mode. Can be any of:\n    - `quant`: Run inference using the layer's quantizer\n    - `approx`: Run inference using approximate product LUT\n    - `noise`: Run inference that is perturbed with additive Gaussian noise\n    \"\"\"\n\n    QUANTIZED = \"Quantized Mode\"\n    NOISE = \"Noise Mode\"\n    APPROXIMATE = \"Approximate Mode\"\n</code></pre>"},{"location":"reference/operators/","title":"Operators","text":"<p>Low-level NN operator implementations for GPU &amp; CPU</p>"},{"location":"reference/operators/#torchapprox.operators.ApproxConv2dOp","title":"<code>ApproxConv2dOp</code>","text":"<p>             Bases: <code>Function</code></p> <p>Autograd wrapper around Im2Col/ApproxGeMM Conv2d operator</p> Source code in <code>src/torchapprox/operators/conv2d.py</code> <pre><code>class ApproxConv2dOp(torch.autograd.Function):\n    \"\"\"\n    Autograd wrapper around Im2Col/ApproxGeMM Conv2d operator\n    \"\"\"\n\n    @staticmethod\n    def forward(\n        x: torch.FloatTensor,\n        w: torch.FloatTensor,\n        quant_params: \"QuantizationParameters\",\n        conv_args: Conv2dArgs,\n        htp_model: Optional[Callable],\n        out_dims: Tuple[int, int],\n        lut: torch.ShortTensor,\n    ):\n        x_q = torch.round((x / quant_params.x_scale) + quant_params.x_zero_point)\n        w_q = torch.round(\n            (w / quant_params.w_scale[:, None, None, None])\n            + quant_params.w_zero_point[:, None, None, None]\n        )\n\n        if htp_model is not None:\n            # HTP model\n            y_q = htp_model(\n                torch.nn.functional.conv2d, x_q, w_q, conv_args.backward_args()\n            )\n            torch.round(y_q)\n        elif conv_args.use_fast_dwconv() and x.is_cuda and w.is_cuda:\n            # Depthwise Conv CUDA Kernel\n            y_q = dwconv2d(x_q, w_q, lut, conv_args.stride, conv_args.padding)\n        else:\n            # im2col &amp; gemm kernel (supports CPU &amp; GPU)\n            y_q = _im2col_conv2d(x_q, w_q, conv_args, lut, out_dims)\n\n        if quant_params.x_zero_point == 0 and torch.all(quant_params.w_zero_point == 0):\n            y_q = _symmetric_requantize(y_q, quant_params)\n        else:\n            y_q = _affine_requantize(\n                x_q,\n                w_q,\n                y_q,\n                quant_params,\n                conv_args,\n                out_dims,\n            )\n\n        y_q = y_q.view(\n            x_q.size(0),\n            conv_args.out_channels,\n            out_dims[0],\n            out_dims[1],\n        )\n\n        return y_q\n\n    @staticmethod\n    def setup_context(ctx: Any, inputs: Tuple[Any], output: Any) -&gt; Any:\n        x, w, _, conv_args, _, _, _ = inputs\n        ctx.save_for_backward(x, w)\n        ctx.conf = conv_args.backward_args()\n\n    @staticmethod\n    def backward(ctx, grad):\n        x, w = ctx.saved_tensors\n        conf = ctx.conf\n        grad_input, grad_weight = _conv_bwd_ste(\n            grad, x, w, conf, ctx.needs_input_grad[0], ctx.needs_input_grad[1]\n        )\n        return grad_input, grad_weight, None, None, None, None, None, None, None\n</code></pre>"},{"location":"reference/operators/#torchapprox.operators.LUTGeMM","title":"<code>LUTGeMM</code>","text":"<p>             Bases: <code>Module</code></p> <p>Class that wraps the Lookup Table matrix multiplication as a torch.nn.Module. This is required so that hooks can be attached in order to trace the quantized and unfolded input/output tensors at runtime</p> Source code in <code>src/torchapprox/operators/lut.py</code> <pre><code>class LUTGeMM(torch.nn.Module):\n    \"\"\"\n    Class that wraps the Lookup Table matrix multiplication as a torch.nn.Module.\n    This is required so that hooks can be attached in order to trace\n    the quantized and unfolded input/output tensors at runtime\n    \"\"\"\n\n    def __init__(self):\n        torch.nn.Module.__init__(self)\n        self._lut: Optional[torch.Tensor] = None\n        self.lut = self.accurate_lut()\n\n    @staticmethod\n    def accurate_lut() -&gt; npt.NDArray[np.int32]:\n        x = np.arange(256)\n        x[x &gt;= 128] -= 256\n        xx, yy = np.meshgrid(x, x)\n        return (xx * yy).astype(np.int32)\n\n    @property\n    def lut(self) -&gt; torch.Tensor:\n        \"\"\"\n        The Lookup table to use for approximate multiplication. LUT can be:\n        - `None`: An accurate product is used internall. This is much faster than passing\n            operands through LUT kernels. Functionally equivalent to running the layer in\n            `quant` mode, but useful when the unfolded inputs/outputs need to be traced at runtime.\n        - `torch.Tensor` or `numpy.array`:\n            - 2D array of size 256x256 is required. Unused entries will be ignored when simulating\n                multiplication where the operand width is less than 8 Bit\n            - When supplying a `torch.Tensor` the datatype needs to be signed 16-Bit.\n        \"\"\"\n        return self._lut\n\n    @lut.setter\n    def lut(self, new_lut: Union[np.ndarray, torch.Tensor]):\n        assert len(new_lut.shape) == 2, \"LUT needs to be 2D square matrix\"\n        assert (\n            new_lut.shape[0] == new_lut.shape[1] == 256\n        ), \"Only 8x8 Bit LUTs are currently supported.\"\n\n        if isinstance(new_lut, torch.Tensor):\n            assert new_lut.dtype == torch.int, \"LUT needs to be signed 32 Bit Integer\"\n            self._lut = new_lut\n        elif isinstance(new_lut, np.ndarray):\n            self._lut = torch.from_numpy(new_lut).contiguous().int()\n        else:\n            raise ValueError(\n                f\"Unknown LUT input type: {type(new_lut)}, supported types: torch.Tensor, np.ndarray\"\n            )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        w: torch.Tensor,\n        quant_params: \"QuantizationParameters\",\n        htp_model: Optional[Callable],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Perform Approximate Matrix Multiply (GeMM)\n\n        Args:\n            x:       Activations Tensor of dimension B x N x M\n            w:       Weight tensor of dimension K x M\n            res:     Pre-allocated output tensor\n\n        Returns:\n            The approximate matrix batched matrix product of x and w, using the supplied LUT\n        \"\"\"\n        return ApproxGeMM.apply(x, w, self.lut, quant_params, htp_model)\n</code></pre>"},{"location":"reference/operators/#torchapprox.operators.LUTGeMM.lut","title":"<code>lut: torch.Tensor</code>  <code>property</code> <code>writable</code>","text":"<p>The Lookup table to use for approximate multiplication. LUT can be: - <code>None</code>: An accurate product is used internall. This is much faster than passing     operands through LUT kernels. Functionally equivalent to running the layer in     <code>quant</code> mode, but useful when the unfolded inputs/outputs need to be traced at runtime. - <code>torch.Tensor</code> or <code>numpy.array</code>:     - 2D array of size 256x256 is required. Unused entries will be ignored when simulating         multiplication where the operand width is less than 8 Bit     - When supplying a <code>torch.Tensor</code> the datatype needs to be signed 16-Bit.</p>"},{"location":"reference/operators/#torchapprox.operators.LUTGeMM.forward","title":"<code>forward(x, w, quant_params, htp_model)</code>","text":"<p>Perform Approximate Matrix Multiply (GeMM)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Activations Tensor of dimension B x N x M</p> required <code>w</code> <code>Tensor</code> <p>Weight tensor of dimension K x M</p> required <code>res</code> <p>Pre-allocated output tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The approximate matrix batched matrix product of x and w, using the supplied LUT</p> Source code in <code>src/torchapprox/operators/lut.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    w: torch.Tensor,\n    quant_params: \"QuantizationParameters\",\n    htp_model: Optional[Callable],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform Approximate Matrix Multiply (GeMM)\n\n    Args:\n        x:       Activations Tensor of dimension B x N x M\n        w:       Weight tensor of dimension K x M\n        res:     Pre-allocated output tensor\n\n    Returns:\n        The approximate matrix batched matrix product of x and w, using the supplied LUT\n    \"\"\"\n    return ApproxGeMM.apply(x, w, self.lut, quant_params, htp_model)\n</code></pre>"},{"location":"reference/utils/","title":"Utilities","text":"<p>TorchApprox Helper functions</p>"},{"location":"reference/utils/#torchapprox.utils.get_approx_modules","title":"<code>get_approx_modules(net)</code>","text":"<p>Retrieve all approximate layers from a model</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>PyTorch neural network model</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, ApproxLayer]]</code> <p>A list of tuples with name and reference to each Approximate layer instance in the model</p> Source code in <code>src/torchapprox/utils/conversion.py</code> <pre><code>def get_approx_modules(net: torch.nn.Module) -&gt; List[Tuple[str, tal.ApproxLayer]]:\n    \"\"\"\n    Retrieve all approximate layers from a model\n\n    Args:\n        net: PyTorch neural network model\n\n    Returns:\n        A list of tuples with name and reference to each Approximate layer instance in the model\n    \"\"\"\n    return [(n, m) for n, m in net.named_modules() if isinstance(m, tal.ApproxLayer)]\n</code></pre>"},{"location":"reference/utils/#torchapprox.utils.wrap_quantizable","title":"<code>wrap_quantizable(net, wrappable_layers=None, qconfig=None)</code>","text":"<p>Performs in-place upgrade of layers in a vanilla PyTorch network to TorchApprox approximate layer implementations. Regular insertion of quant/dequant stubs does not work because the activation quantization parameters are required inside the quantized layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>PyTorch neural network model</p> required <code>wrappable_layers</code> <code>Optional[List[ApproxLayer]]</code> <p>Layer types to be wrapped</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>An identical model with target layers replaced by Approximate Layer implementations</p> Source code in <code>src/torchapprox/utils/conversion.py</code> <pre><code>def wrap_quantizable(\n    net: torch.nn.Module,\n    wrappable_layers: Optional[List[tal.ApproxLayer]] = None,\n    qconfig: Optional[tq.QConfig] = None,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Performs in-place upgrade of layers in a vanilla PyTorch network to TorchApprox\n    approximate layer implementations. Regular insertion of quant/dequant stubs does not work\n    because the activation quantization parameters are required _inside_ the quantized layer.\n\n    Args:\n        net: PyTorch neural network model\n        wrappable_layers: Layer types to be wrapped\n\n    Returns:\n        An identical model with target layers replaced by Approximate Layer implementations\n    \"\"\"\n    if not wrappable_layers:\n        wrappable_layers = [torch.nn.Linear, torch.nn.Conv2d]\n\n    replace_list = []\n\n    def find_replacable_modules(parent_module):\n        if isinstance(parent_module, tal.ApproxWrapper):\n            return\n        for name, child_module in parent_module.named_children():\n            if any([isinstance(child_module, t) for t in wrappable_layers]):\n                replace_list.append((parent_module, name))\n        for child in parent_module.children():\n            find_replacable_modules(child)\n\n    find_replacable_modules(net)\n\n    for parent, name in replace_list:\n        orig_layer = getattr(parent, name)\n        wrapped = tal.ApproxWrapper(orig_layer, qconfig)\n        setattr(parent, name, wrapped)\n    return net\n</code></pre>"}]}